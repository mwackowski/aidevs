{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "query = \"\"\"I have been working on a desktop app project for macOS for a few months now. At this stage, I have approximately 2000 users of this app and I'm the only developer (can't change atm). This success signals that I may need to invest more resources into this project. Currently, I am the only developer of this app. Moreover, this is not my only project; I have several others, which necessitates careful time management and focus. I am faced with the decision of choosing between two paths:\n",
    "\n",
    "The first is about implementing a redesign, which has already been completed. The goal is to improve the overall brand and, specifically, the app's user experience. I plan to fix UI bugs, enhance performance, and add the most-requested features. This may attract more users to the app.\n",
    "\n",
    "The second option is about extending the backend. This will provide me with much more flexibility when implementing even the most advanced features requested by users, although I cannot guarantee they will actually use them. This path would require a larger time investment initially but would improve the development process in the future.\n",
    "\n",
    "Note: \n",
    "- I'm a full-stack designer and full-stack developer. I have broad experience in product development and all business areas.\n",
    "- I'm a solo founder and I'm not looking for a co-founder or team\n",
    "- I'm familiar with all the concepts and tools so feel free to use them\n",
    "\n",
    "Help me decide which path to take by focusing solely on a business context.\"\"\"\n",
    "\n",
    "conversation = [\n",
    "    SystemMessage(\n",
    "        content=\"Act an expert in mental models, critical thinking, and making complex, strategic decisions. Use markdown syntax to format your responses throughout the conversation.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=f\"{query}. Can you brainstorm three different possible strategies that I could take to effectively create new content and do this consistently while maintaining my energy, life balance, and overall quality of the content I produce?  Please be concise, yet detailed as possible.\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = chat(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Certainly! Here are three possible strategies, each taking into account your need for balance and quality.\\n\\n### Strategy 1: Incremental Redesign and Backend Enhancement\\n\\nThis strategy involves doing both tasks in a sequential manner, moving between the two based on priority and user demand.\\n\\n**Step 1:** Start with implementing the most-requested features and fixing the most critical UI bugs that impact the user experience. This could attract more users and increase user satisfaction.\\n\\n**Step 2:** As you're making these incremental changes, start making small enhancements to the backend to support these new features and improvements. This way, you're gradually extending the backend without investing a huge amount of time initially.\\n\\n**Step 3:** Once the most critical UI bugs and features are addressed, focus more on the backend for a while. This will allow you to improve the development process for future features and improvements.\\n\\n### Strategy 2: Parallel Redesign and Backend Enhancement\\n\\nThis strategy involves working on both tasks simultaneously. This requires careful time management but could lead to better long-term outcomes.\\n\\n**Step 1:** Divide your time between redesigning the app and extending the backend. For instance, you could dedicate certain days of the week to each task.\\n\\n**Step 2:** Use user feedback and metrics to prioritize tasks within each area. For the redesign, focus on the changes that will have the greatest impact on user experience. For the backend, focus on the changes that will provide the most flexibility for future development.\\n\\n**Step 3:** Regularly reassess your priorities in each area based on user feedback and the demands of your other projects.\\n\\n### Strategy 3: Outsourcing Backend Enhancement while focusing on Redesign\\n\\nThis strategy involves outsourcing the backend enhancements to a trusted contractor, while you focus on the redesign and other strategic tasks.\\n\\n**Step 1:** Find a trusted contractor or agency who can handle the backend enhancements. Make sure they understand your vision and can work independently.\\n\\n**Step 2:** While they're working on the backend, you can focus on implementing the redesign and improving the user experience. This could attract more users and increase user satisfaction.\\n\\n**Step 3:** Regularly check in with the contractor or agency to make sure the backend enhancements are progressing as expected. This way, you maintain control over the quality of the work.\\n\\nIn all three strategies, it's important to maintain balance and avoid burnout. Make sure to schedule regular breaks and time for relaxation. In addition, consistently seek feedback from users to ensure that your work is meeting their needs and expectations. Finally, don't forget to celebrate your successes along the way!\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Got unknown type For each solution, evaluate their potential, pros and cons, effort needed, difficulty, challenges and expected outcomes. Assign success rate and confidence level for each option.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFor each solution, evaluate their potential, pros and cons, effort needed, difficulty, challenges and expected outcomes. Assign success rate and confidence level for each option.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m conversation\u001b[39m.\u001b[39mappend(HumanMessage(content\u001b[39m=\u001b[39mmessage))\n\u001b[0;32m----> 4\u001b[0m content \u001b[39m=\u001b[39m chat(conversation)\n",
      "File \u001b[0;32m~/aidevs/.venv/lib/python3.10/site-packages/langchain/chat_models/base.py:600\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    594\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    595\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    599\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 600\u001b[0m     generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    601\u001b[0m         [messages], stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    602\u001b[0m     )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    603\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    604\u001b[0m         \u001b[39mreturn\u001b[39;00m generation\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/aidevs/.venv/lib/python3.10/site-packages/langchain/chat_models/base.py:349\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    348\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 349\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    350\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    351\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[1;32m    352\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    353\u001b[0m ]\n\u001b[1;32m    354\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/aidevs/.venv/lib/python3.10/site-packages/langchain/chat_models/base.py:339\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    337\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 339\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    340\u001b[0m                 m,\n\u001b[1;32m    341\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    342\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    343\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    344\u001b[0m             )\n\u001b[1;32m    345\u001b[0m         )\n\u001b[1;32m    346\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    347\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/aidevs/.venv/lib/python3.10/site-packages/langchain/chat_models/base.py:492\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    489\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m     )\n\u001b[1;32m    491\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 492\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    493\u001b[0m         messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/aidevs/.venv/lib/python3.10/site-packages/langchain/chat_models/openai.py:363\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m     stream_iter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream(\n\u001b[1;32m    360\u001b[0m         messages, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    361\u001b[0m     )\n\u001b[1;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m _generate_from_stream(stream_iter)\n\u001b[0;32m--> 363\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_message_dicts(messages, stop)\n\u001b[1;32m    364\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[1;32m    365\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompletion_with_retry(\n\u001b[1;32m    366\u001b[0m     messages\u001b[39m=\u001b[39mmessage_dicts, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    367\u001b[0m )\n",
      "File \u001b[0;32m~/aidevs/.venv/lib/python3.10/site-packages/langchain/chat_models/openai.py:378\u001b[0m, in \u001b[0;36mChatOpenAI._create_message_dicts\u001b[0;34m(self, messages, stop)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`stop` found in both the input and default params.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    377\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m stop\n\u001b[0;32m--> 378\u001b[0m message_dicts \u001b[39m=\u001b[39m [convert_message_to_dict(m) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m messages]\n\u001b[1;32m    379\u001b[0m \u001b[39mreturn\u001b[39;00m message_dicts, params\n",
      "File \u001b[0;32m~/aidevs/.venv/lib/python3.10/site-packages/langchain/chat_models/openai.py:378\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`stop` found in both the input and default params.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    377\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m stop\n\u001b[0;32m--> 378\u001b[0m message_dicts \u001b[39m=\u001b[39m [convert_message_to_dict(m) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m messages]\n\u001b[1;32m    379\u001b[0m \u001b[39mreturn\u001b[39;00m message_dicts, params\n",
      "File \u001b[0;32m~/aidevs/.venv/lib/python3.10/site-packages/langchain/adapters/openai.py:100\u001b[0m, in \u001b[0;36mconvert_message_to_dict\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m     94\u001b[0m     message_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m     95\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mfunction\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     96\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: message\u001b[39m.\u001b[39mcontent,\n\u001b[1;32m     97\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: message\u001b[39m.\u001b[39mname,\n\u001b[1;32m     98\u001b[0m     }\n\u001b[1;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot unknown type \u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m message\u001b[39m.\u001b[39madditional_kwargs:\n\u001b[1;32m    102\u001b[0m     message_dict[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m message\u001b[39m.\u001b[39madditional_kwargs[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: Got unknown type For each solution, evaluate their potential, pros and cons, effort needed, difficulty, challenges and expected outcomes. Assign success rate and confidence level for each option."
     ]
    }
   ],
   "source": [
    "message = \"For each solution, evaluate their potential, pros and cons, effort needed, difficulty, challenges and expected outcomes. Assign success rate and confidence level for each option.\"\n",
    "\n",
    "conversation.append(HumanMessage(content=message))\n",
    "content = chat(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_and_log(message):\n",
    "    conversation.append(HumanMessage(content=message))\n",
    "    content = chat(conversation)\n",
    "    conversation.append(AIMessage(content=content))\n",
    "    return content\n",
    "\n",
    "\n",
    "chat_and_log(\n",
    "    \"For each solution, evaluate their potential, pros and cons, effort needed, difficulty, challenges and expected outcomes. Assign success rate and confidence level for each option.\"\n",
    ")\n",
    "chat_and_log(\n",
    "    \"Extend each solution by deepening the thought process. Generate different scenarios, strategies of implementation that include external resources and how to overcome potential unexpected obstacles.\"\n",
    ")\n",
    "chat_and_log(\n",
    "    \"For each scenario, generate a list of tasks that need to be done to implement the solution.\"\n",
    ")\n",
    "chat_and_log(\n",
    "    \"Based on the evaluations and scenarios, rank the solutions in order. Justify each ranking and offer a final solution.\"\n",
    ")\n",
    "\n",
    "\n",
    "conversationText = \"\"\n",
    "for message in conversation:\n",
    "    conversationText += f\"## {message.type}:\\n\\n{message.content}\\n\\n\"\n",
    "\n",
    "with open(\"bun_python/17_tree/result.md\", \"w\") as f:\n",
    "    f.write(conversationText)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
